 # BERT 언어모델 소개
 
 이전의 방식
 - Seq2Seq
 - Attention + Seq2Seq
 - Transformer

이미지 Autoencoder -> Encoder, Decoder 

BERT -> Transformer Encoder 

BERT는 마스킹을 하게된다. (원본 복원이 더 어렵다.)

순서는 다음과 같다.

- GPT-1
- BERT
- GPT-2

BERT의 모델 구조는

코퍼스양이 굉장히 많다.

토크나이징 한 후 마스킹 과정이 들어간다.

NLP 실험

GLUE datasets -> 언어모델의 성능평가를 정형화된 데이터셋을 활용하면 언어모델의 성능평가가 수월해집니다.

- 단일 문장 분류
- 두 문장 관계 분류
- 문장 토큰 분류
- 기계 독해 정답 분류

네 개의 테스크를 실습을 진행을 할 것이다.

- 단일 문장 분류 -> 감석분석 ( 영화 리뷰 코퍼스를 통해 긍정과 부정을 나누는 작업 )
- 관계 추출 -> 문장 내에서 존재하는 지식 정보를 추출이다.<br>
관계 추출의 대상이 되는 존재를 entity이다. sbj와 obj는 어떤 관계이냐. -> BERT 85점
- 의미 비교 -> 두 문장이 의미적으로 같냐 ? ( 포화지방산이 많은 음식은 ?, 아토피 피부암  -> 0) ( 임신 29주 의 태아의 신장은?, 임신 29주일때 태아의 크기는? -> 1)
- 

