
# '21. 09. 27 NLP 강의 정리

## 1. Competition : " KLUE강의 BERT 언어모델 기반의 단일 분류를 공부하신 이후에 도전 "

### 경진대회 기간이 아닐 때

**1. 대회 개요**

관계 추출(Relation Extraction)은 문장의 단어(Entity)에 대한 속성과 관계를 예측하는 문제입니다.
관계 추출은 지식 그래프 구축을 위한 핵심 구성요소로
(1) 구조화된 검색<br></br>
(2) 감정 분석<br></br>
(3) 질문 답변하기<br></br>
(4) 요약<br></br>
과 같은 자연어처리 응용 프로그램에서 중요합니다. 비구조적인 자연어 문장에서 구조적인 triple을 추출해 정보를 요약하고,
중요한 성분을 핵심적으로 파악할 수 있습니다.

-input : Sentence, subject_entitiy, object_entitiy의 정보를 입력으로 사용 합니다.
-output : relation 30개 중 하나를 예측한 pred_label, 그리고 30개 클래스 각각에 대해 예측한 확률(prob)을 제출해야합니다.
그리고 클래스 별 확률의 순서는 아래처럼 label_list에 있는 클래스 순서와 일치시켜주시기를 부탁드립니다.

**종료**
-10/7 목요일 오후 7시에 대회 종료


## 한국어 언어 모델 학습 및 다중 과제 튜닝
**1. 인공지능의 탄생과 자연어 처리**

-피그말리온과 갈리테이아
 -인간을 대체하는 *인공물*에 대한 최초의 기록

-콜로서스 컴퓨터
 -이미테이션 게임(엘렁튜링): 기계에 지능이 있는지를 판별하고자 하는 실험
 -인간이 보기에 인간같으면 인간에 준하는 지능이 있다고 판단.

-AI의 황금기
 -ELIZA CHAT BOT
 -룰베이스 심리상담 AI
 
-인간의 자연어 처리
 -대화의 단계
 -1. 화자는 자연어 형태로 객체를 인코딩
 -2. 메세지의 전송
 -3. 청자는 자연어를 자연어로 디코딩
 
-컴퓨터의 자연어 처리
 -대화의 단계
 -1. Encoder는 벡터 형태로 자연어를 인코딩
 -2. 메세지의 전송
 -3. Decoder는 벡터형태로 자연어를 디코딩
 
**자연어를 컴퓨터가 이해할 수 있게 수학적으로 어떻게 




**갑자기 BatchNormalization** 출처 eehoeskrap.tistory.com/430
**Gradient Vanishing / Exploding 문제

신경망에서 학습시 Gradient 기반의 방법들은 파라미터 값의 작은 변화가 신경망 출력에 얼마나 영향을 미칠 것인가를 기반으로 파라미터값을 학습시키게 된다.
만약 파라미터 값의 변화가 신경망의 매우 작은 변화를 미치게 될 경우 파라미터를 효과적으로 학습시킬 수 없다.
**Gradient라는 것이 결국 미분값 즉 변화량을 의미하는데 이 변화량이 매우 작아지거나(Vanishing), 커진다면(Exploding) 신경망을 효과적으로 학습시키지 못하고, Error rate가 낮아지지 않고 수렴해 버리는 문제가 발생한다.**

그래서 이러한 문제를 해결하기 위해, Sigmoid나 tanh등의 Activation function은 매우 비선형적인 방식으로 
입력값을 매우 작은 출력 값의 범위로 Squash 해버리는데, 가령 Sigmoid는 실수 범위의 수를 [0,1]로 맵핑한다.
이렇게 출력범위를 설정할 경우, 매우 넓은 입력값의 범위가 극도로 작은 범위의 결과 값으로 매핑된다.
이러한 현상은 비선형 레이어들이 여러개 있을때, 더욱 심해져 학습이 악화된다.
**첫 레이어의 입력 값에 대해 매우 큰 변화량이 있더라도 결과 값의 변화량은 극소가 되어버리는 것**이다.
그래서 이러한 문제점을 해결하기 위해 활성화 함수로 자주 쓰이는 것이 **ReLU(Rectified Linear Unit)**이다.

또 다른 방법들도 존재
-Change activation function: sigmoid to ReLU
-Careful initialization: 가중치 초기화
-Small learning rate: learning rate을 작게함

그러나 이들은 모두 간접적인 방법이고, **학습하는 과정 자체를 전체적으로 안정화**하여 학습속도를 가속시킬수 있는 것이 **배치정규화**이다.

**정규화(normalization)**
기본적으로 정규화 하는 이유는 학습을 빨리하기 위해서 or Local optimum문제에 빠지는 가능성을 줄이기 위해서다.
![image](https://user-images.githubusercontent.com/67318280/134840346-1212167a-4f5c-425d-8f99-a5c09c675fe7.png)

**Internal Covariance Shift**

배치 정규화 논문에서는 학습에서 불안정화가 일어나는 이유를 **Internal Covariance Shift**라고 주장하고 있는데, 이는 네트워크의 각 레이어나 Activation마다 입력값의 분산이 달라지는 현상이다.

